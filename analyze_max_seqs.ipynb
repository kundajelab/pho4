{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras_genomics\n",
    "from keras_genomics.layers.convolutional import RevCompConv1D\n",
    "import keras\n",
    "import keras.layers as kl\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from seqdataloader.batchproducers import coordbased\n",
    "from seqdataloader.batchproducers.coordbased import coordstovals\n",
    "from seqdataloader.batchproducers.coordbased import coordbatchproducers\n",
    "from seqdataloader.batchproducers.coordbased import coordbatchtransformers\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from deeplift.dinuc_shuffle import dinuc_shuffle\n",
    "import json\n",
    "import os\n",
    "import gzip\n",
    "from math import log\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import spearmanr, pearsonr, gaussian_kde\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "def multinomial_nll(true_counts, logits):\n",
    "    \"\"\"Compute the multinomial negative log-likelihood\n",
    "    Args:\n",
    "      true_counts: observed count values\n",
    "      logits: predicted logit values\n",
    "    \"\"\"\n",
    "    counts_per_example = tf.reduce_sum(true_counts, axis=-1)\n",
    "    dist = tfp.distributions.Multinomial(total_count=counts_per_example,\n",
    "                                         logits=logits)\n",
    "    return (-tf.reduce_sum(dist.log_prob(true_counts)) / \n",
    "            tf.to_float(tf.shape(true_counts)[0]))\n",
    "\n",
    "#from https://github.com/kundajelab/basepair/blob/cda0875571066343cdf90aed031f7c51714d991a/basepair/losses.py#L87\n",
    "class MultichannelMultinomialNLL(object):\n",
    "    def __init__(self, n):\n",
    "        self.__name__ = \"MultichannelMultinomialNLL\"\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, true_counts, logits):\n",
    "        for i in range(self.n):\n",
    "            loss = multinomial_nll(true_counts[..., i], logits[..., i])\n",
    "            if i == 0:\n",
    "                total = loss\n",
    "            else:\n",
    "                total += loss\n",
    "        return total\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"n\": self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/amr1/miniconda3/envs/basepair/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-95f01c7c9424>:12: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "with CustomObjectScope({'MultichannelMultinomialNLL': MultichannelMultinomialNLL,'RevCompConv1D': RevCompConv1D}):\n",
    "    model = load_model(\"data/models/k562_max_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lib2_keyToSeq = {}\n",
    "max_rept_keyToSeq = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand_long1,CGCCCCAGCACTGCCAAGCCGACGTTAAAACGGGTTGCTTCATCAATCGAATGTCAATACATAGTCACGTGACACGCGGTAGGCTCGCTATCGGCACTTGCGCTTGAGTGCATCGAATAGTTCGGTTTATGAGCGTCCGGCGGTATGAC\n",
      "GT_AC_45bp,CGCCCCAGCACTGCCAAGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTCACGTGACACACACACACACACACACACACACACACACACACACACACACACAAGTTCGGTTTATGAGCGTCCGGCGGTATGAC\n",
      "GC_GC_repeat,CGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGTCACGTGACGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCCGTCCGGCGGTATGAC\n",
      "GC_AT_30+30+0,CGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGTTGCTTCATCAATCGAATGTCAATACATAGTCACGTGACACGCGGTAGGCTCGCTATCGGCACTTGCGCTATATATATATATATATATATATATATATACGTCCGGCGGTATGAC\n",
      "GA_CT_30+30+0,CGCGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGTTGCTTCATCAATCGAATGTCAATACATAGTCACGTGACACGCGGTAGGCTCGCTATCGGCACTTGCGCCTCTCTCTCTCTCTCTCTCTCTCTCTCTCTCGTCCGGCGGTATGAC\n",
      "GT_scrambled_7,CGCTGGTTTGGTTTGTGGGTGGGGTTTGTTTTGGTGTTTGTTGTGGGTGGTTTGGGTGGGGTTGTCACGTGACAACCCCACCCAAACCACCCACAACAAACACCAAAACAAACCCCACCCACAAACCAAACCACGTCCGGCGGTATGAC\n",
      "Rand_long2,CGCGTTTAACGCGGACGCGTAATCCTTTCCACATCGTGGCGCCTGCCGATCCCGACTGGACTTGTCACGTGACGAATGGAATGGTTCCGACAATAGATCAACAAGAAGTGGTAAGTAGGCTCACATTGCTTTCCGTCCGGCGGTATGAC\n",
      "GT_AC_60bp,CGCGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTCACGTGACACACACACACACACACACACACACACACACACACACACACACACACACACACACACACACCGTCCGGCGGTATGAC\n",
      "AT_AT_repeat,CGCATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATGTCACGTGACATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATCGTCCGGCGGTATGAC\n",
      "GC_AT_45+15+0,CGCGCGCGCGCGCGCGCGCCGACGTTAAAACGGGTTGCTTCATCAATCGAATGTCAATACATAGTCACGTGACACGCGGTAGGCTCGCTATCGGCACTTGCGCTTGAGTGCATCGAATTATATATATATATATCGTCCGGCGGTATGAC\n"
     ]
    }
   ],
   "source": [
    "!head data/experimental/max_lib2_nodegenerate.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motif_and_repeat_strong_5,CGCTGTGTGTGTGTGTGTCACGTGACACACACCACAAACGTCCGGCGGTATGAC\n",
      "Motif_and_repeat_strong_6,CGCATATATATATAGAGTCACGTGACTCTCGCGCGCGCGGTCCGGCGGTATGAC\n",
      "Motif_and_repeat_strong_10,CGCTGTGTGTGTGTGTGTCACGTGACACACAACACACACGTCCGGCGGTATGAC\n",
      "Motif_and_repeat_strong_12,CGCTGTGTGTGTGTGTGTCACGTGACACCACACCACAAAGTCCGGCGGTATGAC\n",
      "Motif_and_repeat_strong_14,CGCGCGCGCGCGCAGAGTCACGTGACTCTATATATATATGTCCGGCGGTATGAC\n",
      "Motif_and_repeat_strong_15,CGCCGCGCGCGCGAGTGTCACGTGACACTATATATATATGTCCGGCGGTATGAC\n",
      "Motif_and_half_repeat_strong_2,CGCACAGGTGTGTGTTGTCACGTGACACACACACACTGTGTCCGGCGGTATGAC\n",
      "Motif_and_half_repeat_strong_10,CGCGTGCGTTGTGTGTGTCACGTGACACACAACTCAACAGTCCGGCGGTATGAC\n",
      "Motif_and_half_repeat_strong_12,CGCAAGGCTTGTGTGTGTCACGTGACACCACACATCGATGTCCGGCGGTATGAC\n",
      "Motif_and_half_repeat_strong_14,CGCAGTGACGCGCAGAGTCACGTGACTCTATATTACGCTGTCCGGCGGTATGAC\n"
     ]
    }
   ],
   "source": [
    "!head data/experimental/max_repetitive_seqs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/experimental/max_lib2_nodegenerate.csv\", encoding='utf-8-sig') as inp:\n",
    "    for line in inp:\n",
    "        max_lib2_keyToSeq[line.strip().split(',')[0]] = line.strip().split(',')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/experimental/max_repetitive_seqs.csv\", encoding='utf-8-sig') as inp:\n",
    "    for line in inp:\n",
    "        max_rept_keyToSeq[line.strip().split(',')[0]] = line.strip().split(',')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapath = \"data/genome/hg19/male.hg19.fa\"\n",
    "GenomeDict={}\n",
    "sequence=''\n",
    "inputdatafile = open(fastapath)\n",
    "for line in inputdatafile:\n",
    "    if line[0]=='>':\n",
    "        if sequence != '':\n",
    "            GenomeDict[chrm] = ''.join(sequence)\n",
    "        chrm = line.strip().split('>')[1]\n",
    "        sequence=[]\n",
    "        Keep=False\n",
    "        continue\n",
    "    else:\n",
    "        sequence.append(line.strip())\n",
    "GenomeDict[chrm] = ''.join(sequence)\n",
    "\n",
    "seq_len = 1346\n",
    "out_pred_len = 1000\n",
    "test_chrms = [\"chr1\", \"chr8\", \"chr21\"]\n",
    "seq_peaks = []\n",
    "with gzip.open(\"data/paralogous/K562_max/idr.optimal_peak.narrowPeak.gz\", 'rt') as inp:\n",
    "    for line in inp:\n",
    "        chrm = line.strip().split('\\t')[0]\n",
    "        if chrm not in test_chrms:\n",
    "            continue\n",
    "        pStart = int(line.strip().split('\\t')[1])\n",
    "        summit = pStart + int(line.strip().split('\\t')[-1])\n",
    "        start = int(summit - (seq_len/2))\n",
    "        end = int(summit + (seq_len/2))\n",
    "        seq_peaks.append(GenomeDict[chrm][start:end].upper())\n",
    "    \n",
    "ltrdict = {\n",
    "           'a':[1,0,0,0],'c':[0,1,0,0],'g':[0,0,1,0],'t':[0,0,0,1],\n",
    "           'n':[0,0,0,0],'A':[1,0,0,0],'C':[0,1,0,0],'G':[0,0,1,0],\n",
    "           'T':[0,0,0,1],'N':[0,0,0,0]}\n",
    "def getOneHot(ISM_sequences):\n",
    "  # takes in list of sequences\n",
    "    one_hot_seqs = []\n",
    "    for seq in ISM_sequences:\n",
    "        one_hot = []\n",
    "        for i in range(len(seq)):\n",
    "            one_hot.append(ltrdict[seq[i:i+1]])\n",
    "        one_hot_seqs.append(one_hot)\n",
    "    return np.array(one_hot_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_into_center(seq, insert):\n",
    "    flank = int((len(seq)-len(insert))/2.0)\n",
    "    new_seq = seq[:flank]+insert+seq[flank+len(insert):]\n",
    "    return new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplift.dinuc_shuffle import dinuc_shuffle\n",
    "num_samples = 100\n",
    "indices = np.random.choice(len(seq_peaks), num_samples, replace=False)\n",
    "yvals_lib2 = {}\n",
    "for key in max_lib2_keyToSeq:\n",
    "    pre_seqs = []\n",
    "    post_seqs = []\n",
    "    for idx in indices:\n",
    "        pre_seq = dinuc_shuffle(seq_peaks[idx])\n",
    "        post_seq = fill_into_center(pre_seq, max_lib2_keyToSeq[key])\n",
    "        pre_seqs.append(pre_seq)\n",
    "        post_seqs.append(post_seq)\n",
    "    pre = model.predict([getOneHot(pre_seqs), np.zeros((num_samples,)), np.zeros((num_samples,out_pred_len,2))])\n",
    "    post = model.predict([getOneHot(post_seqs), np.zeros((num_samples,)), np.zeros((num_samples,out_pred_len,2))])\n",
    "    yvals_lib2[key] = np.mean(post[0]-pre[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvals_rept = {}\n",
    "for key in max_rept_keyToSeq:\n",
    "    pre_seqs = []\n",
    "    post_seqs = []\n",
    "    for idx in indices:\n",
    "        pre_seq = dinuc_shuffle(seq_peaks[idx])\n",
    "        post_seq = fill_into_center(pre_seq, max_rept_keyToSeq[key])\n",
    "        pre_seqs.append(pre_seq)\n",
    "        post_seqs.append(post_seq)\n",
    "    pre = model.predict([getOneHot(pre_seqs), np.zeros((num_samples,)), np.zeros((num_samples,out_pred_len,2))])\n",
    "    post = model.predict([getOneHot(post_seqs), np.zeros((num_samples,)), np.zeros((num_samples,out_pred_len,2))])\n",
    "    yvals_rept[key] = np.mean(post[0]-pre[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/preds/max_k562_lib2_preds.csv','w')\n",
    "for key in max_lib2_keyToSeq:\n",
    "    f.write(key+','+max_lib2_keyToSeq[key]+','+str(yvals_lib2[key])+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand_long1,CGCCCCAGCACTGCCAAGCCGACGTTAAAACGGGTTGCTTCATCAATCGAATGTCAATACATAGTCACGTGACACGCGGTAGGCTCGCTATCGGCACTTGCGCTTGAGTGCATCGAATAGTTCGGTTTATGAGCGTCCGGCGGTATGAC,0.21178478\n",
      "GT_AC_45bp,CGCCCCAGCACTGCCAAGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTCACGTGACACACACACACACACACACACACACACACACACACACACACACACAAGTTCGGTTTATGAGCGTCCGGCGGTATGAC,0.023852415\n",
      "GC_GC_repeat,CGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGTCACGTGACGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCCGTCCGGCGGTATGAC,0.35897782\n",
      "GC_AT_30+30+0,CGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGTTGCTTCATCAATCGAATGTCAATACATAGTCACGTGACACGCGGTAGGCTCGCTATCGGCACTTGCGCTATATATATATATATATATATATATATATACGTCCGGCGGTATGAC,0.32219532\n",
      "GA_CT_30+30+0,CGCGAGAGAGAGAGAGAGAGAGAGAGAGAGAGAGTTGCTTCATCAATCGAATGTCAATACATAGTCACGTGACACGCGGTAGGCTCGCTATCGGCACTTGCGCCTCTCTCTCTCTCTCTCTCTCTCTCTCTCTCGTCCGGCGGTATGAC,0.17793357\n",
      "GT_scrambled_7,CGCTGGTTTGGTTTGTGGGTGGGGTTTGTTTTGGTGTTTGTTGTGGGTGGTTTGGGTGGGGTTGTCACGTGACAACCCCACCCAAACCACCCACAACAAACACCAAAACAAACCCCACCCACAAACCAAACCACGTCCGGCGGTATGAC,0.043573514\n",
      "Rand_long2,CGCGTTTAACGCGGACGCGTAATCCTTTCCACATCGTGGCGCCTGCCGATCCCGACTGGACTTGTCACGTGACGAATGGAATGGTTCCGACAATAGATCAACAAGAAGTGGTAAGTAGGCTCACATTGCTTTCCGTCCGGCGGTATGAC,0.24590974\n",
      "GT_AC_60bp,CGCGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTGTCACGTGACACACACACACACACACACACACACACACACACACACACACACACACACACACACACACACCGTCCGGCGGTATGAC,0.037669726\n",
      "AT_AT_repeat,CGCATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATGTCACGTGACATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATCGTCCGGCGGTATGAC,0.035530735\n",
      "GC_AT_45+15+0,CGCGCGCGCGCGCGCGCGCCGACGTTAAAACGGGTTGCTTCATCAATCGAATGTCAATACATAGTCACGTGACACGCGGTAGGCTCGCTATCGGCACTTGCGCTTGAGTGCATCGAATTATATATATATATATCGTCCGGCGGTATGAC,0.32150826\n"
     ]
    }
   ],
   "source": [
    "!head data/preds/max_lib2_preds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/preds/max_k562_rept_preds.csv','w')\n",
    "for key in max_rept_keyToSeq:\n",
    "    f.write(key+','+max_rept_keyToSeq[key]+','+str(yvals_rept[key])+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motif_and_repeat_strong_5,CGCTGTGTGTGTGTGTGTCACGTGACACACACCACAAACGTCCGGCGGTATGAC,0.09784126\n",
      "Motif_and_repeat_strong_6,CGCATATATATATAGAGTCACGTGACTCTCGCGCGCGCGGTCCGGCGGTATGAC,0.1754628\n",
      "Motif_and_repeat_strong_10,CGCTGTGTGTGTGTGTGTCACGTGACACACAACACACACGTCCGGCGGTATGAC,0.10219929\n",
      "Motif_and_repeat_strong_12,CGCTGTGTGTGTGTGTGTCACGTGACACCACACCACAAAGTCCGGCGGTATGAC,0.08175714\n",
      "Motif_and_repeat_strong_14,CGCGCGCGCGCGCAGAGTCACGTGACTCTATATATATATGTCCGGCGGTATGAC,0.17794918\n",
      "Motif_and_repeat_strong_15,CGCCGCGCGCGCGAGTGTCACGTGACACTATATATATATGTCCGGCGGTATGAC,0.18372864\n",
      "Motif_and_half_repeat_strong_2,CGCACAGGTGTGTGTTGTCACGTGACACACACACACTGTGTCCGGCGGTATGAC,0.07478716\n",
      "Motif_and_half_repeat_strong_10,CGCGTGCGTTGTGTGTGTCACGTGACACACAACTCAACAGTCCGGCGGTATGAC,0.15757151\n",
      "Motif_and_half_repeat_strong_12,CGCAAGGCTTGTGTGTGTCACGTGACACCACACATCGATGTCCGGCGGTATGAC,0.10006472\n",
      "Motif_and_half_repeat_strong_14,CGCAGTGACGCGCAGAGTCACGTGACTCTATATTACGCTGTCCGGCGGTATGAC,0.14299163\n"
     ]
    }
   ],
   "source": [
    "!head data/preds/max_rept_preds.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basepair]",
   "language": "python",
   "name": "conda-env-basepair-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
